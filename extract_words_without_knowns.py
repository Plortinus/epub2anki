import spacy
from ebooklib import epub
from bs4 import BeautifulSoup
from collections import Counter
import csv
import sys
import time
import threading
import itertools
import re

# åŠ è½½è¥¿è¯­ NLP æ¨¡å‹
nlp = spacy.load("es_core_news_sm")

# è½¬åœˆè¿›åº¦æ¡
def spinner_task(stop_event, progress):
    spinner = itertools.cycle(['|', '/', '-', '\\'])
    while not stop_event.is_set():
        pct = progress[0]
        sys.stdout.write(f"\rå¤„ç†ä¸­... {next(spinner)} {pct:.1f}%")
        sys.stdout.flush()
        time.sleep(0.1)
    sys.stdout.write("\râœ… å¤„ç†å®Œæˆï¼          \n")

# è¯»å–å·²å­¦å•è¯
def load_known_words(csv_path):
    known_words = set()
    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            word = row["term"].strip()
            if word:
                known_words.add(word.lower())
    return known_words

# åŠ è½½åœç”¨è¯
def load_stopwords(path="stopwords_es.txt"):
    stopwords = set()
    with open(path, encoding="utf-8") as f:
        for line in f:
            word = line.strip().lower()
            if word:
                stopwords.add(word)
    return stopwords

# æå–å¥å­ï¼ˆæ¯è¡Œæ‹†åˆ† + spaCy åˆ†å¥ + æ‰¹é‡å¤„ç†ï¼‰
def extract_sentences_from_epub(epub_path):
    book = epub.read_epub(epub_path)
    texts = []
    for item in book.get_items():
        if item.get_type() == 9:  # æ–‡æœ¬ç±»å‹ (XHTML)
            soup = BeautifulSoup(item.get_body_content(), "html.parser")
            texts.append(soup.get_text())

    full_text = "\n".join(texts).strip()

    # æŒ‰æ¢è¡Œæ‹†åˆ†ä¸ºæ½œåœ¨å¥å­
    lines = [line.strip() for line in full_text.splitlines() if line.strip()]
    potential_sentences = []

    # æ‰¹é‡å¤„ç† spaCy åˆ†å¥
    for doc in nlp.pipe(lines, batch_size=50):
        for sent in doc.sents:
            s = sent.text.strip()
            if s:
                potential_sentences.append(s)

    return potential_sentences

def extract_minimal_sentences(epub_path, known_csv_path, stopwords_path, output_csv):
    known_words = load_known_words(known_csv_path)
    stopwords = load_stopwords(stopwords_path)

    stop_event = threading.Event()
    progress = [0.0]
    spinner_thread = threading.Thread(target=spinner_task, args=(stop_event, progress))
    spinner_thread.start()

    try:
        sentences = extract_sentences_from_epub(epub_path)
        total_sentences = len(sentences)

        word_counter = Counter()
        word_sentence_candidates = {}

        # ä½¿ç”¨ nlp.pipe æ‰¹é‡å¤„ç†å•è¯ç»Ÿè®¡
        docs = list(nlp.pipe(sentences, batch_size=50))
        for idx, doc in enumerate(docs, 1):
            words = [token.text for token in doc if token.is_alpha]
            for w in words:
                lw = w.lower()
                if lw in known_words or lw in stopwords:
                    continue
                word_counter[w] += 1
                if w not in word_sentence_candidates:
                    word_sentence_candidates[w] = doc.text

            progress[0] = idx / total_sentences * 100

        # åˆ†é…å¥å­ï¼šä½é¢‘ä¼˜å…ˆ
        word_sentence = {}
        used_sentences = set()

        for word, freq in sorted(word_counter.items(), key=lambda x: x[1]):  # ä»ä½é¢‘åˆ°é«˜é¢‘
            candidate = word_sentence_candidates.get(word, "")
            if candidate and candidate not in used_sentences:
                word_sentence[word] = candidate
                used_sentences.add(candidate)
            else:
                word_sentence[word] = ""  # æ²¡æœ‰æ–°å¥å­ â†’ ç•™ç©º

        # è¾“å‡º CSV
        with open(output_csv, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["sentence", "words"])
            for sentence in used_sentences:
                words_in_sentence = [w for w, s in word_sentence.items() if s == sentence]
                writer.writerow([sentence, ", ".join(words_in_sentence)])

        # ----------------- ç»Ÿè®¡ä¿¡æ¯ -----------------
        # åŸä¹¦æ€»è¯æ•°
        original_words = []
        for doc in docs:
            original_words.extend([token.text for token in doc if token.is_alpha])
        original_total_words = len(original_words)

        # æŠ½å–å¥å­æ€»è¯æ•°
        selected_words = []
        for sentence in used_sentences:
            doc = nlp(sentence)
            selected_words.extend([token.text for token in doc if token.is_alpha])
        selected_total_words = len(selected_words)

        # å‹ç¼©æ¯”ç‡
        compression_ratio = selected_total_words / original_total_words * 100 if original_total_words > 0 else 0
        unique_sentences = len(used_sentences)

        print("\nğŸ“Š ç»Ÿè®¡ç»“æœï¼š")
        print(f"åŸä¹¦æ€»è¯æ•°: {original_total_words}")
        print(f"æŠ½å–å¥å­æ€»è¯æ•°: {selected_total_words}")
        print(f"å‹ç¼©æ¯”ç‡: {compression_ratio:.2f}%")
        print(f"æœ€ç»ˆä¿ç•™å¥å­æ•°: {unique_sentences}")

    finally:
        stop_event.set()
        spinner_thread.join()

if __name__ == "__main__":
    extract_minimal_sentences(
        "your_book.epub",     # è¾“å…¥ EPUB
        "lingqs.csv",         # å·²å­¦å•è¯åˆ—è¡¨
        "stopwords_es.txt",   # åœç”¨è¯åˆ—è¡¨
        "minimal_sentences.csv"  # è¾“å‡º CSV
    )