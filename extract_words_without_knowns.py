from ebooklib import epub
from bs4 import BeautifulSoup
from collections import Counter
import re
import csv
import sys
import time
import threading
import itertools

# è½¬åœˆ + ç™¾åˆ†æ¯”æ˜¾ç¤º
def spinner_task(stop_event, progress):
    spinner = itertools.cycle(['|', '/', '-', '\\'])
    while not stop_event.is_set():
        pct = progress[0]
        sys.stdout.write(f"\rå¤„ç†ä¸­... {next(spinner)} {pct:.1f}%")
        sys.stdout.flush()
        time.sleep(0.1)
    sys.stdout.write("\râœ… å¤„ç†å®Œæˆï¼          \n")

# è¯»å–å·²å­¦å•è¯
def load_known_words(csv_path):
    known_words = set()
    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            word = row["term"].strip()
            if word:
                known_words.add(word.lower())
    return known_words

# åŠ è½½åœç”¨è¯
def load_stopwords(path="stopwords_es.txt"):
    stopwords = set()
    with open(path, encoding="utf-8") as f:
        for line in f:
            word = line.strip().lower()
            if word:
                stopwords.add(word)
    return stopwords

def extract_minimal_sentences(epub_path, known_csv_path, stopwords_path, output_csv):
    known_words = load_known_words(known_csv_path)
    stopwords = load_stopwords(stopwords_path)

    stop_event = threading.Event()
    progress = [0.0]
    spinner_thread = threading.Thread(target=spinner_task, args=(stop_event, progress))
    spinner_thread.start()

    try:
        # è¯»å– epub
        book = epub.read_epub(epub_path)
        texts = []
        for item in book.get_items():
            if item.get_type() == 9:  # æ–‡æœ¬ç±»å‹ (XHTML)
                soup = BeautifulSoup(item.get_body_content(), "html.parser")
                texts.append(soup.get_text())

        full_text = " ".join(texts)
        sentences = re.split(r'(?<=[.!?])\s+', full_text)
        total_sentences = len(sentences)

        word_counter = Counter()
        word_sentence_candidates = {}

        # ç»Ÿè®¡å•è¯é¢‘ç‡ & æ”¶é›†å€™é€‰å¥å­
        for idx, sentence in enumerate(sentences, 1):
            clean_sentence = ' '.join(sentence.replace("\n", " ").replace("\r", " ").split())
            clean_sentence = re.sub(r'^[\-\â€”\â€“\~\s]+', '', clean_sentence)
            clean_sentence = re.sub(r'[<>]+', '', clean_sentence)
            clean_sentence = re.sub(r'^[â€œ"\'â€˜]+|[â€"\'â€™]+$', '', clean_sentence)

            words = re.findall(r"[a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±ÃÃ‰ÃÃ“ÃšÃœÃ‘]+", clean_sentence)
            for w in words:
                lw = w.lower()
                if lw in known_words or lw in stopwords:
                    continue
                word_counter[w] += 1
                # ä¿å­˜ç¬¬ä¸€ä¸ªå€™é€‰å¥å­
                if w not in word_sentence_candidates:
                    word_sentence_candidates[w] = clean_sentence

            progress[0] = idx / total_sentences * 100

        # åˆ†é…å¥å­ï¼šä»ä½é¢‘è¯å¼€å§‹
        word_sentence = {}
        used_sentences = set()

        for word, freq in sorted(word_counter.items(), key=lambda x: x[1]):  # ä»ä½é¢‘åˆ°é«˜é¢‘
            candidate = word_sentence_candidates.get(word, "")
            if candidate and candidate not in used_sentences:
                word_sentence[word] = candidate
                used_sentences.add(candidate)
            else:
                word_sentence[word] = ""  # æ²¡æœ‰æ–°å¥å­ â†’ ç•™ç©º

        # æœ€ç»ˆåªè¾“å‡ºå»é‡åçš„å¥å­åˆ—è¡¨
        with open(output_csv, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["sentence", "words"])
            for sentence in used_sentences:
                words_in_sentence = [w for w, s in word_sentence.items() if s == sentence]
                writer.writerow([sentence, ", ".join(words_in_sentence)])

        # ---- ç»Ÿè®¡ä¿¡æ¯ ----
        total_words = len(word_counter)
        covered_words = sum(1 for s in word_sentence.values() if s)
        unique_sentences = len(used_sentences)
        avg_words_per_sentence = covered_words / unique_sentences if unique_sentences > 0 else 0

        print("\nğŸ“Š ç»Ÿè®¡ç»“æœï¼š")
        print(f"æ€»å•è¯æ•°: {total_words}")
        print(f"è¦†ç›–å•è¯æ•°: {covered_words}")
        print(f"æœ€ç»ˆä¿ç•™å¥å­æ•°: {unique_sentences}")
        print(f"å¹³å‡æ¯å¥è¦†ç›–å•è¯æ•°: {avg_words_per_sentence:.2f}")

    finally:
        stop_event.set()
        spinner_thread.join()

if __name__ == "__main__":
    extract_minimal_sentences(
        "atomic.epub",
        "lingqs.csv",
        "stopwords_es.txt",
        "atomic.csv"
    )