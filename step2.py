import re
import csv
from ebooklib import epub
from bs4 import BeautifulSoup
from collections import Counter
from tqdm import tqdm
import spacy

# ================= åˆå§‹åŒ– =================
nlp = spacy.load("es_core_news_sm")  # è½»é‡è¥¿è¯­æ¨¡å‹

# -----------------------------
# è¯»å–å·²çŸ¥å•è¯
# -----------------------------
def load_known_words(path):
    words = set()
    try:
        with open(path, encoding="utf-8") as f:
            reader = csv.reader(f)
            for row in reader:
                if row:
                    words.add(row[0].strip().lower())
    except FileNotFoundError:
        print(f"âš ï¸ æ–‡ä»¶ {path} æœªæ‰¾åˆ°ï¼Œè¿”å›ç©ºé›†åˆ")
    return words

# -----------------------------
# åŠ è½½åœç”¨è¯
# -----------------------------
def load_stopwords(path):
    stopwords = set()
    try:
        with open(path, encoding="utf-8") as f:
            for line in f:
                w = line.strip().lower()
                if w:
                    stopwords.add(w)
    except FileNotFoundError:
        print(f"âš ï¸ æ–‡ä»¶ {path} æœªæ‰¾åˆ°ï¼Œè¿”å›ç©ºé›†åˆ")
    return stopwords

# -----------------------------
# æå– EPUB æ–‡æœ¬
# -----------------------------
def extract_text_from_epub(epub_path):
    book = epub.read_epub(epub_path)
    texts = []
    for item in book.get_items():
        if item.get_type() == 9:  # æ–‡æœ¬ç±»å‹
            soup = BeautifulSoup(item.get_body_content(), "html.parser")
            texts.append(soup.get_text())
    return "\n".join(texts)

# -----------------------------
# ä¸»å‡½æ•°
# -----------------------------
def extract_unknown_words_with_pos(epub_path, known_csv, stopwords_txt, output_csv):
    known_words = load_known_words(known_csv)
    stopwords = load_stopwords(stopwords_txt)

    print("ğŸ“Œ æå–æ–‡æœ¬...")
    full_text = extract_text_from_epub(epub_path)

    print("ğŸ“Œ åˆ†è¯...")
    words = re.findall(r'\b[^\W\d_]+\b', full_text, flags=re.UNICODE)
    words = [w.lower() for w in words]

    print("ğŸ“Œ ç­›é€‰ç”Ÿè¯...")
    unknown_words = [w for w in words if w not in known_words and w not in stopwords]

    counter = Counter(unknown_words)
    sorted_words = sorted(counter.items(), key=lambda x: x[1], reverse=True)
    print(f"ğŸ“Œ ç”Ÿè¯æ•°é‡: {len(sorted_words)}")

    # ================= è·å–è¯æ€§ =================
    print("ğŸ“Œ è·å–è¯æ€§...")
    # æ‰¹é‡å¤„ç†ï¼Œå‡å°‘å†…å­˜å‹åŠ›
    all_unknowns = [word for word, freq in sorted_words]
    word_pos_map = {}
    for i in tqdm(range(0, len(all_unknowns), 5000)):
        batch = all_unknowns[i:i+5000]
        doc = nlp(" ".join(batch))
        for token in doc:
            word_pos_map[token.text.lower()] = token.pos_

    # ================= å†™å…¥ CSV =================
    print(f"ğŸ’¾ è¾“å‡º CSV: {output_csv}")
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["word", "frequency", "pos"])
        for word, freq in sorted_words:
            pos = word_pos_map.get(word, "X")  # å¦‚æœæ²¡æ‰¾åˆ°è¯æ€§ï¼Œæ ‡è®°ä¸º X
            writer.writerow([word, freq, pos])

# -----------------------------
# è„šæœ¬å…¥å£
# -----------------------------
if __name__ == "__main__":
    extract_unknown_words_with_pos(
        "deepwork.epub",
        "anki_words_list.csv",
        "stopwords_es.txt",
        "deepwork_unknown_words_pos.csv"
    )